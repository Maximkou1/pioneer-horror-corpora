{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbGFbDaACQn9",
        "outputId": "94cef447-f6e6-4dc6-8212-73b23d1d75f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (6.0.0)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "! pip install conllu\n",
        "! python -m spacy download ru_core_news_sm -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# загружаем русскую модель\n",
        "nlp = spacy.load('ru_core_news_sm')"
      ],
      "metadata": {
        "id": "2dLgX807Cye5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import open\n",
        "from conllu import parse_incr\n",
        "\n",
        "# читаем коннлу файл\n",
        "data_file = open(\"conllu_stories.conllu\", \"r\", encoding=\"utf-8\")\n",
        "sent_list = [tokenlist for tokenlist in parse_incr(data_file)]"
      ],
      "metadata": {
        "id": "I0N4U1KECaet"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция, которая парсит запрос пользователя и возвращает список условий для поиска\n",
        "def parse_query(query):\n",
        "    conditions = []\n",
        "    # делим по пробелам\n",
        "    query_parts = query.split()\n",
        "    # для каждой части запроса\n",
        "    for part in query_parts:\n",
        "        # если запрос в кавычках\n",
        "        if part.startswith('\"') and part.endswith('\"'):\n",
        "            # достаём и записываем как словоформу\n",
        "            form = part.strip('\"')\n",
        "            conditions.append({\"form\": form})\n",
        "        # если все буквы заглавные\n",
        "        elif part.isupper():\n",
        "            # то это часть речи\n",
        "            upos = part\n",
        "            conditions.append({\"upos\": upos})\n",
        "        # если в строке есть +\n",
        "        elif \"+\" in part:\n",
        "            # то лемма и часть речи\n",
        "            lemma, upos = part.split(\"+\")\n",
        "            conditions.append({\"lemma\": lemma, \"upos\": upos})\n",
        "        # в ином случае\n",
        "        else:\n",
        "            # находим лемму введённого слова и записываем её\n",
        "            lemma = nlp(part)[0].lemma_\n",
        "            conditions.append({\"lemma\": lemma})\n",
        "    return conditions  # возвращаем условия\n",
        "\n",
        "# собственно функция поиска\n",
        "def search_conllu_by_chain(conllu_text, query):\n",
        "    # определяем необходимые условия запроса\n",
        "    conditions = parse_query(query)\n",
        "    results = []\n",
        "\n",
        "    # проходимся по предложениям\n",
        "    for sentence in conllu_text:\n",
        "      # достаём название страшилки, id предложения и текст предложения из метаданных\n",
        "      text_title, sent_id, sent_text = list(sentence.metadata.values())\n",
        "\n",
        "      # итерируемся по последовательным токенам в зависимости от длины цепочки\n",
        "      for i in range(len(sentence) - len(conditions) + 1):\n",
        "          match = True\n",
        "          for j, condition in enumerate(conditions):\n",
        "              token = sentence[i + j]\n",
        "\n",
        "              # проверяем на соответствие каждому из требуемых условий\n",
        "              if \"lemma\" in condition and token.get(\"lemma\") != condition[\"lemma\"]:\n",
        "                  # если несоответствует - прерываем\n",
        "                  match = False\n",
        "                  break\n",
        "              if \"upos\" in condition and token.get(\"upostag\") != condition[\"upos\"]:\n",
        "                  match = False\n",
        "                  break\n",
        "              if \"form\" in condition and token.get(\"form\") != condition[\"form\"]:\n",
        "                  match = False\n",
        "                  break\n",
        "\n",
        "          # если все условия соблдюдены\n",
        "          if match:\n",
        "              # сохраняем форму, лемму и pos-тег\n",
        "              matched_tokens = [(token.get(\"form\"), token.get(\"lemma\"), token.get(\"upostag\")) for token in sentence[i:i + len(conditions)]]\n",
        "              # и добавляем к результатам название, id и текст предложения\n",
        "              results.append((text_title, sent_id, sent_text, matched_tokens))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "UIEjiTwiCb04"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# запрос пользователя\n",
        "query = 'прийти NUM NOUN'"
      ],
      "metadata": {
        "id": "SYyGbSCBDBNW"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ищем цепочку по запросу\n",
        "results = search_conllu_by_chain(sent_list, query)\n",
        "\n",
        "# печатаем результаты\n",
        "print(f\"Результаты поиска по запросу '{query}':\")\n",
        "for text_title, sent_id, sent_text, matched_tokens in results:\n",
        "    matched_str = \" -> \".join([f'\"{form}\" ({lemma}, {upos})' for form, lemma, upos in matched_tokens])\n",
        "    print(f'В страшилке \"{text_title}\" нашлось {matched_str}: \\n{sent_text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GotgArIKDF5F",
        "outputId": "966b1b9f-4dc5-4d05-f136-64f6927fa997"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты поиска по запросу 'прийти NUM NOUN':\n",
            "В страшилке \"История про Барби\" нашлось \"пришла\" (прийти, VERB) -> \"одна\" (одна, NUM) -> \"женщина\" (женщина, NOUN): \n",
            "И к ним пришла одна женщина с дочкой, ну, их подруга.\n",
            "В страшилке \"Красный бантик\" нашлось \"Пришли\" (прийти, VERB) -> \"два\" (два, NUM) -> \"милиционера\" (милиционер, NOUN): \n",
            "Пришли два милиционера и остались.\n"
          ]
        }
      ]
    }
  ]
}